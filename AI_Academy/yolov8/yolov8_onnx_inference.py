import os
import cv2
import numpy as np
import onnxruntime as ort
import glob
from pathlib import Path

class YOLOv8ONNXSegmentationInference:
    def __init__(self, model_path, test_folder_path):
        """
        YOLOv8 ONNX ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡  í´ë˜ìŠ¤
        
        Args:
            model_path (str): ONNX ëª¨ë¸ ê²½ë¡œ (best.onnx)
            test_folder_path (str): í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ
        """
        self.model_path = model_path
        self.test_folder_path = test_folder_path
        self.session = None
        self.input_name = None
        self.output_names = None
        self.load_model()
    
    def load_model(self):
        """ONNX ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
        
        print(f"ğŸ“¥ ONNX ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
        self.session = ort.InferenceSession(self.model_path, providers=['CPUExecutionProvider'])
        
        # ì…ì¶œë ¥ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        print("âœ… ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"   ì…ë ¥: {self.session.get_inputs()[0].shape}")
        for i, output in enumerate(self.session.get_outputs()):
            print(f"   ì¶œë ¥ {i}: {output.shape}")
    
    def get_test_images(self):
        """í…ŒìŠ¤íŠ¸ í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°"""
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
        image_files = []
        
        for ext in image_extensions:
            pattern = os.path.join(self.test_folder_path, ext)
            image_files.extend(glob.glob(pattern))
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´
            pattern_upper = os.path.join(self.test_folder_path, ext.upper())
            image_files.extend(glob.glob(pattern_upper))
        
        image_files = list(set(image_files))  # ì¤‘ë³µ ì œê±°
        print(f"ğŸ“ ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        
        return sorted(image_files)
    
    def preprocess_image(self, image):
        """
        ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (YOLOv8 ONNXìš©)
        
        Args:
            image (np.array): ì›ë³¸ ì´ë¯¸ì§€ (BGR)
        
        Returns:
            np.array: ì „ì²˜ë¦¬ëœ í…ì„œ (1, 3, 640, 640)
        """
        # BGR -> RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 640x640ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(image_rgb, (640, 640))
        
        # ì •ê·œí™”: 0-255 -> 0-1
        normalized = resized.astype(np.float32) / 255.0
        
        # HWC -> CHW
        transposed = np.transpose(normalized, (2, 0, 1))
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (3, 640, 640) -> (1, 3, 640, 640)
        input_tensor = np.expand_dims(transposed, axis=0)
        
        return input_tensor
    
    def postprocess_detection(self, outputs, orig_w, orig_h, conf_threshold=0.5):
        """
        ONNX ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬
        
        Args:
            outputs: ëª¨ë¸ ì¶œë ¥ [detection, proto_masks]
            orig_w, orig_h: ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
        
        Returns:
            tuple: (final_mask, detection_count)
        """
        # Detection ì¶œë ¥ ì²˜ë¦¬
        detection_output = outputs[0]  # (1, 37, 8400)
        mask_protos = outputs[1] if len(outputs) > 1 else None  # (1, 32, 160, 160)
        
        # Detection ì¬êµ¬ì„±
        detection = detection_output[0].T  # (8400, 37)
        
        # ë°•ìŠ¤, ì‹ ë¢°ë„, ë§ˆìŠ¤í¬ ê³„ìˆ˜ ë¶„ë¦¬
        boxes = detection[:, :4]  # (8400, 4) - x_center, y_center, width, height
        class_confidences = detection[:, 4]  # (8400,) - í´ë˜ìŠ¤ ì‹ ë¢°ë„
        mask_coeffs = detection[:, 5:] if detection.shape[1] > 5 else None  # (8400, 32)
        
        # ì ì‘í˜• ì„ê³„ê°’ìœ¼ë¡œ ìœ íš¨í•œ ê²€ì¶œ ì°¾ê¸°
        thresholds = [0.001, 0.005, 0.01, 0.05, 0.4]
        best_results = None
        best_threshold = conf_threshold
        
        for threshold in thresholds:
            valid_mask = class_confidences > threshold
            valid_count = valid_mask.sum()
            
            if valid_count > 0 and valid_count < 100:
                best_results = valid_mask
                best_threshold = threshold
                break
        
        # ìœ íš¨í•œ ê²€ì¶œì´ ì—†ìœ¼ë©´ ìƒìœ„ 5ê°œë¼ë„ ì‹œë„
        if best_results is None or best_results.sum() == 0:
            top_5_indices = np.argsort(class_confidences)[::-1][:5]
            best_results = np.zeros(len(class_confidences), dtype=bool)
            best_results[top_5_indices] = True
            best_threshold = class_confidences[top_5_indices[-1]] if len(top_5_indices) > 0 else 0.001
        
        detection_count = best_results.sum()
        
        if detection_count == 0 or mask_protos is None or mask_coeffs is None:
            return np.zeros((orig_h, orig_w), dtype=np.uint8), 0
        
        # ìœ íš¨í•œ ê²€ì¶œ ì¶”ì¶œ
        valid_boxes = boxes[best_results]
        valid_confidences = class_confidences[best_results]
        valid_mask_coeffs = mask_coeffs[best_results]
        
        # NMS ì ìš© (ì„ íƒì )
        if len(valid_confidences) > 1:
            # ì¢Œí‘œ ë³€í™˜: center -> x1,y1,x2,y2
            x_center, y_center = valid_boxes[:, 0], valid_boxes[:, 1]
            width, height = valid_boxes[:, 2], valid_boxes[:, 3]
            
            x1 = (x_center - width / 2) * (orig_w / 640)
            y1 = (y_center - height / 2) * (orig_h / 640)
            x2 = (x_center + width / 2) * (orig_w / 640)
            y2 = (y_center + height / 2) * (orig_h / 640)
            
            # NMSìš© ë°•ìŠ¤ í˜•ì‹: [x, y, width, height]
            boxes_for_nms = np.column_stack([x1, y1, x2 - x1, y2 - y1])
            nms_indices = cv2.dnn.NMSBoxes(
                boxes_for_nms.tolist(),
                valid_confidences.tolist(),
                best_threshold,
                0.4  # IoU threshold
            )
            
            if len(nms_indices) > 0:
                nms_indices = nms_indices.flatten()
                valid_mask_coeffs = valid_mask_coeffs[nms_indices]
                detection_count = len(nms_indices)
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        final_mask = np.zeros((orig_h, orig_w), dtype=np.uint8)
        proto_masks = mask_protos[0]  # (32, 160, 160)
        
        for i in range(len(valid_mask_coeffs)):
            # ë§ˆìŠ¤í¬ ê³„ìˆ˜ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
            coeffs = valid_mask_coeffs[i]  # (32,)
            
            # í”„ë¡œí† íƒ€ì… ë§ˆìŠ¤í¬ì™€ ê³„ìˆ˜ ê³±í•˜ê¸°
            mask = np.zeros((160, 160), dtype=np.float32)
            for j in range(32):
                mask += coeffs[j] * proto_masks[j]
            
            # Sigmoid í™œì„±í™” ë° ì´ì§„í™”
            mask = 1.0 / (1.0 + np.exp(-mask))
            mask_binary = (mask > 0.5).astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            mask_resized = cv2.resize(mask_binary, (orig_w, orig_h))
            
            # ìµœì¢… ë§ˆìŠ¤í¬ì— ëˆ„ì 
            final_mask = np.maximum(final_mask, mask_resized * 255)
        
        return final_mask, detection_count
    
    def create_blue_overlay(self, image, mask, alpha=0.3, blue_color=(255, 0, 0)):
        """
        ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ì— íˆ¬ëª…í•œ íŒŒë€ìƒ‰ ì˜¤ë²„ë ˆì´ ìƒì„±
        
        Args:
            image (np.array): ì›ë³¸ ì´ë¯¸ì§€
            mask (np.array): ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬
            alpha (float): íˆ¬ëª…ë„ (0.0-1.0)
            blue_color (tuple): BGR íŒŒë€ìƒ‰ ê°’
        
        Returns:
            np.array: ì˜¤ë²„ë ˆì´ê°€ ì ìš©ëœ ì´ë¯¸ì§€
        """
        overlay_image = image.copy()
        
        if mask is not None and np.count_nonzero(mask) > 0:
            # íŒŒë€ìƒ‰ ë ˆì´ì–´ ìƒì„±
            mask_colored = np.zeros_like(image)
            mask_colored[mask > 0] = blue_color  # BGR í˜•ì‹ íŒŒë€ìƒ‰
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            overlay_image = cv2.addWeighted(overlay_image, 1-alpha, mask_colored, alpha, 0)
        
        return overlay_image
    
    def process_single_image(self, image_path, save_result=True, show_result=True):
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬
        
        Args:
            image_path (str): ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            save_result (bool): ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            show_result (bool): ê²°ê³¼ í™”ë©´ í‘œì‹œ ì—¬ë¶€
        
        Returns:
            tuple: (overlay_image, should_exit, detection_info)
        """
        print(f"\nğŸ” ì²˜ë¦¬ ì¤‘: {os.path.basename(image_path)}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None, False, None
        
        orig_h, orig_w = image.shape[:2]
        print(f"   ì´ë¯¸ì§€ í¬ê¸°: {orig_w} x {orig_h}")
        
        # ì „ì²˜ë¦¬
        input_tensor = self.preprocess_image(image)
        
        # ONNX ì¶”ë¡ 
        outputs = self.session.run(self.output_names, {self.input_name: input_tensor})
        
        # í›„ì²˜ë¦¬
        mask, detection_count = self.postprocess_detection(outputs, orig_w, orig_h)
        
        mask_pixels = np.count_nonzero(mask)
        detection_info = {
            'detections': detection_count,
            'mask_pixels': mask_pixels,
            'has_lane': mask_pixels > 1000  # 1000í”½ì…€ ì´ìƒì´ë©´ ìœ íš¨í•œ ë ˆì¸ìœ¼ë¡œ íŒë‹¨
        }
        
        if detection_info['has_lane']:
            print(f"âœ… ë ˆì¸ ê²€ì¶œ ì„±ê³µ: {detection_count}ê°œ ê°ì²´, {mask_pixels:,}í”½ì…€")
            overlay_image = self.create_blue_overlay(image, mask)
        else:
            print(f"âš ï¸  ë ˆì¸ ê²€ì¶œ ë¯¸ë¯¸: {detection_count}ê°œ ê°ì²´, {mask_pixels}í”½ì…€")
            overlay_image = image
        
        # ê²°ê³¼ ì €ì¥
        if save_result:
            output_dir = os.path.join(os.path.dirname(image_path), '..', 'onnx_results')
            os.makedirs(output_dir, exist_ok=True)
            
            filename = os.path.basename(image_path)
            name, ext = os.path.splitext(filename)
            
            # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ì €ì¥
            overlay_path = os.path.join(output_dir, f"{name}_onnx_segmented{ext}")
            cv2.imwrite(overlay_path, overlay_image)
            
            # ë§ˆìŠ¤í¬ë§Œ ì €ì¥
            mask_path = os.path.join(output_dir, f"{name}_mask{ext}")
            cv2.imwrite(mask_path, mask)
            
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {overlay_path}")
            print(f"ğŸ’¾ ë§ˆìŠ¤í¬ ì €ì¥: {mask_path}")
        
        # ê²°ê³¼ í™”ë©´ í‘œì‹œ
        should_exit = False
        if show_result:
            should_exit = self.display_result_cv2(image, overlay_image, 
                                                 os.path.basename(image_path), 
                                                 detection_info)
        
        return overlay_image, should_exit, detection_info
    
    def display_result_cv2(self, original, result, title, detection_info):
        """
        OpenCVë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ í‘œì‹œ
        
        Args:
            original (np.array): ì›ë³¸ ì´ë¯¸ì§€
            result (np.array): ì²˜ë¦¬ëœ ì´ë¯¸ì§€
            title (str): ì œëª©
            detection_info (dict): ê²€ì¶œ ì •ë³´
        """
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (í™”ë©´ì— ë§ê²Œ)
        height, width = original.shape[:2]
        max_height = 600
        if height > max_height:
            scale = max_height / height
            new_width = int(width * scale)
            new_height = int(height * scale)
            original_resized = cv2.resize(original, (new_width, new_height))
            result_resized = cv2.resize(result, (new_width, new_height))
        else:
            original_resized = original
            result_resized = result
        
        # ì´ë¯¸ì§€ë¥¼ ë‚˜ë€íˆ ë°°ì¹˜
        combined = np.hstack([original_resized, result_resized])
        
        # í…ìŠ¤íŠ¸ ì¶”ê°€
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(combined, f'Original: {title}', (10, 30), font, 0.7, (255, 255, 255), 2)
        
        # ê²€ì¶œ ê²°ê³¼ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€ê²½
        status_color = (0, 255, 0) if detection_info['has_lane'] else (0, 165, 255)  # ì´ˆë¡ ë˜ëŠ” ì£¼í™©
        status_text = f"Lane Detected: {detection_info['detections']} objs, {detection_info['mask_pixels']:,} pixels"
        cv2.putText(combined, status_text, (original_resized.shape[1] + 10, 30), 
                   font, 0.6, status_color, 2)
        
        # í•˜ë‹¨ì— ì•ˆë‚´ ë©”ì‹œì§€
        cv2.putText(combined, 'Press SPACE for next image, ESC to exit', 
                   (10, combined.shape[0] - 40), font, 0.6, (0, 255, 255), 2)
        cv2.putText(combined, 'Press S to save individual result', 
                   (10, combined.shape[0] - 15), font, 0.6, (0, 255, 255), 2)
        
        # ì°½ í‘œì‹œ
        cv2.imshow('YOLOv8 ONNX Lane Segmentation (Blue)', combined)
        
        # í‚¤ ì…ë ¥ ëŒ€ê¸°
        status_emoji = "âœ…" if detection_info['has_lane'] else "âš ï¸"
        print(f"ğŸ‘† {status_emoji} {title} - SPACE: ë‹¤ìŒ, ESC: ì¢…ë£Œ, S: ê°œë³„ì €ì¥")
        
        while True:
            key = cv2.waitKey(0) & 0xFF
            if key == 27:  # ESC
                return True
            elif key == ord(' '):  # SPACE
                return False
            elif key == ord('s') or key == ord('S'):  # Sí‚¤ë¡œ ê°œë³„ ì €ì¥
                timestamp = __import__('datetime').datetime.now().strftime("%Y%m%d_%H%M%S")
                save_path = f"manual_save_{timestamp}_{title}"
                cv2.imwrite(save_path, combined)
                print(f"ğŸ“¸ ìˆ˜ë™ ì €ì¥: {save_path}")
                continue
    
    def process_all_images(self, max_images=None, save_results=True, show_results=True):
        """
        í…ŒìŠ¤íŠ¸ í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬
        
        Args:
            max_images (int): ì²˜ë¦¬í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            save_results (bool): ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            show_results (bool): ê²°ê³¼ í™”ë©´ í‘œì‹œ ì—¬ë¶€
        """
        image_files = self.get_test_images()
        
        if not image_files:
            print("âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if max_images:
            image_files = image_files[:max_images]
        
        print(f"\nğŸš€ ONNX ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘: {len(image_files)}ê°œ")
        print("=" * 60)
        
        if show_results:
            print("ğŸ’¡ ì¡°ì‘ë²•:")
            print("   - SPACE: ë‹¤ìŒ ì´ë¯¸ì§€")
            print("   - ESC: ì¢…ë£Œ")
            print("   - S: í˜„ì¬ ê²°ê³¼ ìˆ˜ë™ ì €ì¥")
            print("=" * 60)
        
        # í†µê³„ ì •ë³´
        stats = {
            'processed': 0,
            'successful_detections': 0,
            'total_objects': 0,
            'total_pixels': 0
        }
        
        try:
            for i, image_path in enumerate(image_files, 1):
                try:
                    print(f"\n[{i}/{len(image_files)}]", end=" ")
                    result, should_exit, detection_info = self.process_single_image(
                        image_path, 
                        save_result=save_results,
                        show_result=show_results
                    )
                    
                    if result is not None:
                        stats['processed'] += 1
                        if detection_info and detection_info['has_lane']:
                            stats['successful_detections'] += 1
                            stats['total_objects'] += detection_info['detections']
                            stats['total_pixels'] += detection_info['mask_pixels']
                    
                    # ESCë¡œ ì¢…ë£Œ
                    if should_exit:
                        print("\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¢…ë£Œë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
                        break
                        
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
        
        finally:
            cv2.destroyAllWindows()
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ í†µê³„:")
        print(f"   ğŸ“Š ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {stats['processed']}/{len(image_files)}")
        print(f"   âœ… ì„±ê³µì  ê²€ì¶œ: {stats['successful_detections']}/{stats['processed']}")
        if stats['successful_detections'] > 0:
            print(f"   ğŸ¯ í‰ê·  ê°ì²´ ìˆ˜: {stats['total_objects']/stats['successful_detections']:.1f}")
            print(f"   ğŸ¨ í‰ê·  í”½ì…€ ìˆ˜: {stats['total_pixels']/stats['successful_detections']:,.0f}")
        
        success_rate = (stats['successful_detections'] / stats['processed']) * 100 if stats['processed'] > 0 else 0
        print(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    BASE_PATH = r"C:\Project\DSS\AI_Academy\yolov8"
    MODEL_PATH = os.path.join(BASE_PATH, "DSS_AI_training", "DSS_experiment_1", "weights", "onnx_models", "best.onnx")
    TEST_FOLDER = os.path.join(BASE_PATH, "dataset", "test", "images")
    
    print("=" * 60)
    print("ğŸ¯ YOLOv8 ONNX ì°¨ì„  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡  ì‹œìŠ¤í…œ (íŒŒë€ìƒ‰ í‘œì‹œ)")
    print("=" * 60)
    print(f"ğŸ“‚ ONNX ëª¨ë¸: {MODEL_PATH}")
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ í´ë”: {TEST_FOLDER}")
    print("=" * 60)
    
    # ê²½ë¡œ í™•ì¸
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        print("ONNX ë³€í™˜ì´ ì™„ë£Œëœ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    if not os.path.exists(TEST_FOLDER):
        print(f"âŒ í…ŒìŠ¤íŠ¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {TEST_FOLDER}")
        return
    
    try:
        # ONNX ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        inference = YOLOv8ONNXSegmentationInference(MODEL_PATH, TEST_FOLDER)
        
        # 10ê°œ ì´ë¯¸ì§€ë¡œ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        inference.process_all_images(
            max_images=10,      # ì²˜ìŒ 10ê°œ ì´ë¯¸ì§€ë§Œ
            save_results=False,  # ê²°ê³¼ ìë™ ì €ì¥
            show_results=True   # í™”ë©´ì— í‘œì‹œ
        )
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()

# ===== ì‚¬ìš© ì˜ˆì‹œ =====
"""
# 1. ê¸°ë³¸ ì‚¬ìš©ë²• (10ê°œ ì´ë¯¸ì§€)
python yolov8_onnx_inference.py

# 2. ì»¤ìŠ¤í…€ ì‚¬ìš©ë²•
inference = YOLOv8ONNXSegmentationInference(model_path, test_folder)
inference.process_all_images(max_images=20)  # 20ê°œ ì´ë¯¸ì§€
inference.process_single_image("specific_image.jpg")  # íŠ¹ì • ì´ë¯¸ì§€ í•˜ë‚˜

# 3. ìë™ ì €ì¥ë§Œ (í™”ë©´ í‘œì‹œ ì—†ì´)
inference.process_all_images(max_images=50, show_results=False)
"""